{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was tested on an NVIDIA Tesla K80 GPU, although we anticipate that similar hardware should also work.\r\n",
        "The ONNX inference runtime was tested on CPU and was optimized for memory consumption through quantization. If memory\r\n",
        "is not an issue, using BERT instead of DistilBERT and skipping the quantization step should lead to performance gains.\r\n",
        "Parts of this notebook are adapted from resources from ChrisMcCormickAI.\r\n",
        "\r\n",
        "The training data used to train this iteration of the text quality watchdog model was collected from CommonCrawl. The full\r\n",
        "dataset used to train the production model contains 18,000 text examples. The sample training dataset provided contains 100 text\r\n",
        "examples, and is intended solely for demonstrative purposes.\r\n",
        "\r\n",
        "Standard Python packages used: Pandas, Numpy, Tensorflow, Pytorch, Tokenizers, Transformers, Matplotlib, Seaborn, Scikit-learn, Onnx, Onnxruntime.<br>\r\n",
        "Modules from the Python Standard Library: re, time, datetime, random"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Training Data**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The training dataset contains the following fields:\r\n",
        "#\r\n",
        "# text => the raw text extracted from each document during document cracking\r\n",
        "# score => the oov score assigned to each example, score range [0,1] indicates % of out-of-vocab words\r\n",
        "# filename => filename of original document for downstream data analysis\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "print('Parsing the dataset .tsv file')\r\n",
        "\r\n",
        "data = pd.read_csv('train_data_sample.tsv', sep='\\t')\r\n",
        "\r\n",
        "print('    Done.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing the dataset .tsv file\n",
            "    Done.\n"
          ]
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052770309
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect To Hardware Accelerator**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\r\n",
        "\r\n",
        "# Get the GPU device name.\r\n",
        "device_name = tf.test.gpu_device_name()\r\n",
        "\r\n",
        "# The device name should look like the following:\r\n",
        "if device_name == '/device:GPU:0':\r\n",
        "    print('Found GPU at: {}'.format(device_name))\r\n",
        "else:\r\n",
        "    raise SystemError('GPU device not found')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052772136
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\r\n",
        "\r\n",
        "# If there's a GPU available...\r\n",
        "if torch.cuda.is_available():    \r\n",
        "\r\n",
        "    # Tell PyTorch to use the GPU.    \r\n",
        "    device = torch.device(\"cuda\")\r\n",
        "\r\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\r\n",
        "\r\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\r\n",
        "\r\n",
        "# If not...\r\n",
        "else:\r\n",
        "    print('No GPU available, using the CPU instead.')\r\n",
        "    device = torch.device(\"cpu\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA Tesla K80\n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052772594
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess Data**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "\r\n",
        "tokenizer = BertWordPieceTokenizer(\"./bert-base-uncased-vocab.txt\")\r\n",
        "tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\", length=128)\r\n",
        "tokenizer.enable_truncation(max_length=128)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052772861
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\r\n",
        "import re\r\n",
        "\r\n",
        "# Tokenize text input and compute attention masks for each training example\r\n",
        "\r\n",
        "input_ids = []\r\n",
        "attention_masks = []\r\n",
        "\r\n",
        "for text_example in data.text:\r\n",
        "\r\n",
        "    # Explicitly truncate text to 128 whitespace-separated tokens\r\n",
        "    space_idx = [space.start() for space in re.finditer(' ', text_example)]\r\n",
        "    if len(space_idx) >= 128:\r\n",
        "        end_idx = space_idx[127]\r\n",
        "        text_example = (text_example[:end_idx])\r\n",
        "\r\n",
        "    encoding = tokenizer.encode(text_example)\r\n",
        "\r\n",
        "    input_ids.append(encoding.ids)\r\n",
        "    attention_masks.append(encoding.attention_mask)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052773248
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect training labels\r\n",
        "\r\n",
        "labels = data.score.values\r\n",
        "labels = [1 if score > 0.8 else 0 for score in labels]  # binarize the labels with classification threshold of 0.8"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052773662
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Use 90% for training and 10% for validation.\r\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \r\n",
        "                                                            random_state=2018, test_size=0.1)\r\n",
        "# Do the same for the masks.\r\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\r\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052774338
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \r\n",
        "# for our model.\r\n",
        "train_inputs = torch.tensor(train_inputs)\r\n",
        "validation_inputs = torch.tensor(validation_inputs)\r\n",
        "\r\n",
        "train_labels = torch.tensor(train_labels)\r\n",
        "validation_labels = torch.tensor(validation_labels)\r\n",
        "\r\n",
        "train_masks = torch.tensor(train_masks)\r\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052774546
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \r\n",
        "# here.\r\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\r\n",
        "# 16 or 32.\r\n",
        "\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# Create the DataLoader for our training set.\r\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# Create the DataLoader for our validation set.\r\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\r\n",
        "validation_sampler = SequentialSampler(validation_data)\r\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\r\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052774833
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize Model**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig, DistilBertModel\r\n",
        "\r\n",
        "# Load DistilBertForSequenceClassification, the pretrained DistilBERT model with a single \r\n",
        "# linear classification layer on top. \r\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\r\n",
        "    \"distilbert-base-uncased\", # Use the 12-layer DistilBERT model, with an uncased vocab.\r\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification. \r\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\r\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\r\n",
        ")\r\n",
        "\r\n",
        "# Tell pytorch to run this model on the GPU.\r\n",
        "model.cuda()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052778230
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \r\n",
        "optimizer = AdamW(model.parameters(),\r\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5.\r\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\r\n",
        "                )"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052778628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "# Number of training epochs (authors recommend between 2 and 4)\r\n",
        "epochs = 4\r\n",
        "\r\n",
        "# Total number of training steps is number of batches * number of epochs.\r\n",
        "total_steps = len(train_dataloader) * epochs\r\n",
        "\r\n",
        "# Create the learning rate scheduler.\r\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \r\n",
        "                                            num_warmup_steps = 0,\r\n",
        "                                            num_training_steps = total_steps)"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052778817
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation Utility Functions**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\r\n",
        "def flat_accuracy(preds, labels):\r\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\r\n",
        "    labels_flat = labels.flatten()\r\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052779002
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\r\n",
        "import datetime\r\n",
        "\r\n",
        "def format_time(elapsed):\r\n",
        "    '''\r\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\r\n",
        "    '''\r\n",
        "    # Round to the nearest second.\r\n",
        "    elapsed_rounded = int(round((elapsed)))\r\n",
        "    \r\n",
        "    # Format as hh:mm:ss\r\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052779267
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\r\n",
        "\r\n",
        "# This training code is based on the `run_glue.py` script here:\r\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\r\n",
        "\r\n",
        "\r\n",
        "# Set the seed value all over the place to make this reproducible.\r\n",
        "seed_val = 42\r\n",
        "\r\n",
        "random.seed(seed_val)\r\n",
        "np.random.seed(seed_val)\r\n",
        "torch.manual_seed(seed_val)\r\n",
        "torch.cuda.manual_seed_all(seed_val)\r\n",
        "\r\n",
        "# Store the average loss after each epoch so we can plot them.\r\n",
        "loss_values = []\r\n",
        "\r\n",
        "# For each epoch...\r\n",
        "for epoch_i in range(0, epochs):\r\n",
        "    \r\n",
        "    # ========================================\r\n",
        "    #               Training\r\n",
        "    # ========================================\r\n",
        "    \r\n",
        "    # Perform one full pass over the training set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\r\n",
        "    print('Training...')\r\n",
        "\r\n",
        "    # Measure how long the training epoch takes.\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Reset the total loss for this epoch.\r\n",
        "    total_loss = 0\r\n",
        "\r\n",
        "    # Put the model into training mode. Don't be mislead--the call to \r\n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\r\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\r\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    # For each batch of training data...\r\n",
        "    for step, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        # Progress update every 40 batches.\r\n",
        "        if step % 40 == 0 and not step == 0:\r\n",
        "            # Calculate elapsed time in minutes.\r\n",
        "            elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "            # Report progress.\r\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\r\n",
        "\r\n",
        "        # Unpack this training batch from our dataloader. \r\n",
        "        #\r\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \r\n",
        "        # `to` method.\r\n",
        "        #\r\n",
        "        # `batch` contains three pytorch tensors:\r\n",
        "        #   [0]: input ids \r\n",
        "        #   [1]: attention masks\r\n",
        "        #   [2]: labels \r\n",
        "        b_input_ids = batch[0].to(device)\r\n",
        "        b_input_mask = batch[1].to(device)\r\n",
        "        b_labels = batch[2].to(device)\r\n",
        "\r\n",
        "        # Always clear any previously calculated gradients before performing a\r\n",
        "        # backward pass. PyTorch doesn't do this automatically because \r\n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \r\n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\r\n",
        "        model.zero_grad()        \r\n",
        "\r\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\r\n",
        "        # This will return the loss (rather than the model output) because we\r\n",
        "        # have provided the `labels`.\r\n",
        "        # The documentation for this `model` function is here: \r\n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "        outputs = model(b_input_ids, \r\n",
        "                    attention_mask=b_input_mask, \r\n",
        "                    labels=b_labels)\r\n",
        "        \r\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \r\n",
        "        # loss value out of the tuple.\r\n",
        "        loss = outputs[0]\r\n",
        "\r\n",
        "        # Accumulate the training loss over all of the batches so that we can\r\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\r\n",
        "        # single value; the `.item()` function just returns the Python value \r\n",
        "        # from the tensor.\r\n",
        "        total_loss += loss.item()\r\n",
        "\r\n",
        "        # Perform a backward pass to calculate the gradients.\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # Clip the norm of the gradients to 1.0.\r\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # Update parameters and take a step using the computed gradient.\r\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\r\n",
        "        # modified based on their gradients, the learning rate, etc.\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # Update the learning rate.\r\n",
        "        scheduler.step()\r\n",
        "\r\n",
        "    # Calculate the average loss over the training data.\r\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \r\n",
        "    \r\n",
        "    # Store the loss value for plotting the learning curve.\r\n",
        "    loss_values.append(avg_train_loss)\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\r\n",
        "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "        \r\n",
        "    # ========================================\r\n",
        "    #               Validation\r\n",
        "    # ========================================\r\n",
        "    # After the completion of each training epoch, measure our performance on\r\n",
        "    # our validation set.\r\n",
        "\r\n",
        "    print(\"\")\r\n",
        "    print(\"Running Validation...\")\r\n",
        "\r\n",
        "    t0 = time.time()\r\n",
        "\r\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\r\n",
        "    # during evaluation.\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    # Tracking variables \r\n",
        "    eval_loss, eval_accuracy = 0, 0\r\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\r\n",
        "\r\n",
        "    # Evaluate data for one epoch\r\n",
        "    for batch in validation_dataloader:\r\n",
        "        \r\n",
        "        # Add batch to GPU\r\n",
        "        batch = tuple(t.to(device) for t in batch)\r\n",
        "        \r\n",
        "        # Unpack the inputs from our dataloader\r\n",
        "        b_input_ids, b_input_mask, b_labels = batch\r\n",
        "        \r\n",
        "        # Telling the model not to compute or store gradients, saving memory and\r\n",
        "        # speeding up validation\r\n",
        "        with torch.no_grad():        \r\n",
        "\r\n",
        "            # Forward pass, calculate logit predictions.\r\n",
        "            # This will return the logits rather than the loss because we have\r\n",
        "            # not provided labels.\r\n",
        "            # token_type_ids is the same as the \"segment ids\", which \r\n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\r\n",
        "            # The documentation for this `model` function is here: \r\n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\r\n",
        "            outputs = model(b_input_ids, \r\n",
        "                            attention_mask=b_input_mask)\r\n",
        "        \r\n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\r\n",
        "        # values prior to applying an activation function like the softmax.\r\n",
        "        logits = outputs[0]\r\n",
        "\r\n",
        "        # Move logits and labels to CPU\r\n",
        "        logits = logits.detach().cpu().numpy()\r\n",
        "        label_ids = b_labels.to('cpu').numpy()\r\n",
        "        \r\n",
        "        # Calculate the accuracy for this batch of test sentences.\r\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\r\n",
        "\r\n",
        "        # Accumulate the total accuracy.\r\n",
        "        eval_accuracy += tmp_eval_accuracy\r\n",
        "\r\n",
        "        # Track the number of batches\r\n",
        "        nb_eval_steps += 1\r\n",
        "\r\n",
        "    # Report the final accuracy for this validation run.\r\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\r\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\r\n",
        "\r\n",
        "print(\"\")\r\n",
        "print(\"Training complete!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0:00:02\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052786145
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Model**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If following this notebook cell-for-cell, the observed decrease in the loss function will be lower than\r\n",
        "# that of the production model due to the small size of the sample training dataset used for demo purposes.\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Use plot styling from seaborn.\r\n",
        "sns.set(style='darkgrid')\r\n",
        "\r\n",
        "# Increase the plot size and font size.\r\n",
        "sns.set(font_scale=1.5)\r\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\r\n",
        "\r\n",
        "# Plot the learning curve.\r\n",
        "plt.plot(loss_values, 'b-o')\r\n",
        "\r\n",
        "# Label the plot.\r\n",
        "plt.title(\"Training loss\")\r\n",
        "plt.xlabel(\"Epoch\")\r\n",
        "plt.ylabel(\"Loss\")\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 864x432 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052786552
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on validation set\r\n",
        "\r\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(validation_inputs)))\r\n",
        "\r\n",
        "# Put model in evaluation mode\r\n",
        "model.eval()\r\n",
        "\r\n",
        "# Tracking variables \r\n",
        "predictions , true_labels = [], []\r\n",
        "\r\n",
        "# Predict \r\n",
        "for batch in validation_dataloader:\r\n",
        "  # Add batch to GPU\r\n",
        "  batch = tuple(t.to(device) for t in batch)\r\n",
        "  \r\n",
        "  # Unpack the inputs from our dataloader\r\n",
        "  b_input_ids, b_input_mask, b_labels = batch\r\n",
        "  \r\n",
        "  # Telling the model not to compute or store gradients, saving memory and \r\n",
        "  # speeding up prediction\r\n",
        "  with torch.no_grad():\r\n",
        "      # Forward pass, calculate logit predictions\r\n",
        "      outputs = model(b_input_ids,\r\n",
        "                      attention_mask=b_input_mask)\r\n",
        "\r\n",
        "  logits = outputs[0]\r\n",
        "\r\n",
        "  # Move logits and labels to CPU\r\n",
        "  logits = logits.detach().cpu().numpy()\r\n",
        "  label_ids = b_labels.to('cpu').numpy()\r\n",
        "  \r\n",
        "  # Store predictions and true labels\r\n",
        "  predictions.append(logits)\r\n",
        "  true_labels.append(label_ids)\r\n",
        "\r\n",
        "print('    DONE.')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting labels for 10 test sentences...\n",
            "    DONE.\n"
          ]
        }
      ],
      "execution_count": 17,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052787130
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import matthews_corrcoef\r\n",
        "\r\n",
        "matthews_set = []\r\n",
        "\r\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\r\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\r\n",
        "\r\n",
        "# For each input batch...\r\n",
        "for i in range(len(true_labels)):\r\n",
        "  \r\n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \r\n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\r\n",
        "  # in to a list of 0s and 1s.\r\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\r\n",
        "  \r\n",
        "  # Calculate and store the coef for this batch.  \r\n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \r\n",
        "  matthews_set.append(matthews)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Matthews Corr. Coef. for each batch...\n"
          ]
        }
      ],
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052787332
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If following this notebook cell-for-cell, the observed predictive performance will be different than\r\n",
        "# that of the production model due to the small size of the sample training dataset used for demo purposes.\r\n",
        "#\r\n",
        "# At this cell, the production Pytorch model achieves 0.932 MCC\r\n",
        "\r\n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\r\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\r\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\r\n",
        "\r\n",
        "# Combine the correct labels for each batch into a single list.\r\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\r\n",
        "\r\n",
        "# Calculate the MCC\r\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\r\n",
        "\r\n",
        "print('MCC: %.3f' % mcc)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 1.000\n"
          ]
        }
      ],
      "execution_count": 19,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052787518
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Pytorch model\r\n",
        "torch.save(model, './watchdog_model_demo.pt')"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052790494
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Convert Model to ONNX Runtime***"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('./watchdog_model_demo.pt')\r\n",
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "DistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (1): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (2): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (3): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (4): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (5): TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052805397
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper method to prepare sample inputs for ONNX\r\n",
        "\r\n",
        "def get_ids_and_masks(text):\r\n",
        "    tokenizer = BertWordPieceTokenizer(\"./bert-base-uncased-vocab.txt\")\r\n",
        "    tokenizer.enable_padding(pad_id=0, pad_token=\"[PAD]\", length=128)\r\n",
        "    tokenizer.enable_truncation(max_length=128)\r\n",
        "\r\n",
        "    input_ids = []\r\n",
        "    attention_masks = []\r\n",
        "\r\n",
        "    encoding = tokenizer.encode(text)\r\n",
        "    input_ids.append(encoding.ids)\r\n",
        "    attention_masks.append(encoding.attention_mask)\r\n",
        "\r\n",
        "    return input_ids, attention_masks"
      ],
      "outputs": [],
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052805607
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sample inputs and attention mask\r\n",
        "\r\n",
        "sample_text = data.text[0]\r\n",
        "\r\n",
        "sample_input_ids, sample_attention_masks = get_ids_and_masks(sample_text)\r\n",
        "\r\n",
        "sample_input_ids = torch.tensor(sample_input_ids)\r\n",
        "sample_attention_masks = torch.tensor(sample_attention_masks)\r\n",
        "\r\n",
        "sample_input_ids = sample_input_ids.to(device)\r\n",
        "sample_attention_masks = sample_attention_masks.to(device)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052805808
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.onnx import export\r\n",
        "\r\n",
        "model_onnx_path = \"watchdog_model_demo.onnx\"\r\n",
        "\r\n",
        "dummy_input = (\r\n",
        "    sample_input_ids,\r\n",
        "    sample_attention_masks\r\n",
        ")\r\n",
        "input_names = [\"input_ids\", \"attention_mask\"]\r\n",
        "output_names = [\"logits\"]\r\n",
        "\r\n",
        "export(\r\n",
        "    model, dummy_input, model_onnx_path, input_names = input_names, \r\n",
        "    output_names = output_names\r\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/transformers/modeling_utils.py:1790: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  input_tensor.shape[chunk_dim] == tensor_shape for input_tensor in input_tensors\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052815321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantize ONNX Model for Efficient Runtime**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\r\n",
        "from onnxruntime.quantization import QuantizationMode, quantize\r\n",
        "\r\n",
        "# Load the ONNX model\r\n",
        "onnx_model = onnx.load(\"watchdog_model_demo.onnx\")\r\n",
        "\r\n",
        "quantized_model = quantize(\r\n",
        "    model=onnx_model,\r\n",
        "    quantization_mode=QuantizationMode.IntegerOps,\r\n",
        "    force_fusions=True,\r\n",
        "    symmetric_weight=True\r\n",
        ")\r\n",
        "\r\n",
        "quantized_model_path = \"watchdog_model_quantized_demo.onnx\"\r\n",
        "\r\n",
        "# Save model\r\n",
        "onnx.save_model(quantized_model, quantized_model_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: onnxruntime.quantization.quantize is deprecated.\n",
            "         Please use quantize_static for static quantization, quantize_dynamic for dynamic quantization.\n",
            "Warning: The original model opset version is 9, which does not support quantization. Please update the model to opset >= 11. Updating the model automatically to opset 11. Please verify the quantized model.\n"
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052831121
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantized Performance Check**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\r\n",
        "\r\n",
        "# Load the ONNX model\r\n",
        "model = onnx.load(\"watchdog_model_quantized_demo.onnx\")\r\n",
        "\r\n",
        "# Check that the IR is well formed\r\n",
        "onnx.checker.check_model(model)\r\n",
        "\r\n",
        "# Print a human readable representation of the graph\r\n",
        "onnx.helper.printable_graph(model.graph)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "'graph torch-jit-export (\\n  %input_ids[INT64, 1x128]\\n  %attention_mask[INT64, 1x128]\\n) initializers (\\n  %distilbert.embeddings.LayerNorm.weight[FLOAT, 768]\\n  %distilbert.embeddings.LayerNorm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.0.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.0.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.0.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.0.output_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.1.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.1.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.1.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.1.output_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.2.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.2.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.2.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.2.output_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.3.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.3.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.3.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.3.output_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.4.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.4.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.4.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.4.output_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.attention.q_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.attention.k_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.attention.v_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.attention.out_lin.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.sa_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.5.sa_layer_norm.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.ffn.lin1.bias[FLOAT, 3072]\\n  %distilbert.transformer.layer.5.ffn.lin2.bias[FLOAT, 768]\\n  %distilbert.transformer.layer.5.output_layer_norm.weight[FLOAT, 768]\\n  %distilbert.transformer.layer.5.output_layer_norm.bias[FLOAT, 768]\\n  %pre_classifier.bias[FLOAT, 768]\\n  %classifier.bias[FLOAT, 2]\\n  %822[INT64, 1]\\n  %823[INT64, 1]\\n  %824[INT64, 1]\\n  %826[INT64, 1]\\n  %827[INT64, 1]\\n  %828[INT64, 1]\\n  %830[INT64, 1]\\n  %831[INT64, 1]\\n  %832[INT64, 1]\\n  %833[INT64, 1]\\n  %834[INT64, 1]\\n  %835[INT64, 1]\\n  %836[INT64, 1]\\n  %841[INT64, 1]\\n  %842[INT64, 1]\\n  %843[INT64, 1]\\n  %845[INT64, 1]\\n  %846[INT64, 1]\\n  %847[INT64, 1]\\n  %849[INT64, 1]\\n  %850[INT64, 1]\\n  %851[INT64, 1]\\n  %852[INT64, 1]\\n  %853[INT64, 1]\\n  %854[INT64, 1]\\n  %855[INT64, 1]\\n  %860[INT64, 1]\\n  %861[INT64, 1]\\n  %862[INT64, 1]\\n  %864[INT64, 1]\\n  %865[INT64, 1]\\n  %866[INT64, 1]\\n  %868[INT64, 1]\\n  %869[INT64, 1]\\n  %870[INT64, 1]\\n  %871[INT64, 1]\\n  %872[INT64, 1]\\n  %873[INT64, 1]\\n  %874[INT64, 1]\\n  %879[INT64, 1]\\n  %880[INT64, 1]\\n  %881[INT64, 1]\\n  %883[INT64, 1]\\n  %884[INT64, 1]\\n  %885[INT64, 1]\\n  %887[INT64, 1]\\n  %888[INT64, 1]\\n  %889[INT64, 1]\\n  %890[INT64, 1]\\n  %891[INT64, 1]\\n  %892[INT64, 1]\\n  %893[INT64, 1]\\n  %898[INT64, 1]\\n  %899[INT64, 1]\\n  %900[INT64, 1]\\n  %902[INT64, 1]\\n  %903[INT64, 1]\\n  %904[INT64, 1]\\n  %906[INT64, 1]\\n  %907[INT64, 1]\\n  %908[INT64, 1]\\n  %909[INT64, 1]\\n  %910[INT64, 1]\\n  %911[INT64, 1]\\n  %912[INT64, 1]\\n  %917[INT64, 1]\\n  %918[INT64, 1]\\n  %919[INT64, 1]\\n  %921[INT64, 1]\\n  %922[INT64, 1]\\n  %923[INT64, 1]\\n  %925[INT64, 1]\\n  %926[INT64, 1]\\n  %927[INT64, 1]\\n  %928[INT64, 1]\\n  %929[INT64, 1]\\n  %930[INT64, 1]\\n  %931[INT64, 1]\\n  %distilbert.embeddings.word_embeddings.weight_quantized[INT8, 30522x768]\\n  %distilbert.embeddings.word_embeddings.weight_scale[FLOAT, scalar]\\n  %distilbert.embeddings.word_embeddings.weight_zero_point[INT8, scalar]\\n  %distilbert.embeddings.position_embeddings.weight_quantized[INT8, 512x768]\\n  %distilbert.embeddings.position_embeddings.weight_scale[FLOAT, scalar]\\n  %distilbert.embeddings.position_embeddings.weight_zero_point[INT8, scalar]\\n  %821_quantized[INT8, 768x768]\\n  %821_scale[FLOAT, scalar]\\n  %821_zero_point[INT8, scalar]\\n  %825_quantized[INT8, 768x768]\\n  %825_scale[FLOAT, scalar]\\n  %825_zero_point[INT8, scalar]\\n  %829_quantized[INT8, 768x768]\\n  %829_scale[FLOAT, scalar]\\n  %829_zero_point[INT8, scalar]\\n  %837_quantized[INT8, 768x768]\\n  %837_scale[FLOAT, scalar]\\n  %837_zero_point[INT8, scalar]\\n  %838_quantized[INT8, 768x3072]\\n  %838_scale[FLOAT, scalar]\\n  %838_zero_point[INT8, scalar]\\n  %839_quantized[INT8, 3072x768]\\n  %839_scale[FLOAT, scalar]\\n  %839_zero_point[INT8, scalar]\\n  %840_quantized[INT8, 768x768]\\n  %840_scale[FLOAT, scalar]\\n  %840_zero_point[INT8, scalar]\\n  %844_quantized[INT8, 768x768]\\n  %844_scale[FLOAT, scalar]\\n  %844_zero_point[INT8, scalar]\\n  %848_quantized[INT8, 768x768]\\n  %848_scale[FLOAT, scalar]\\n  %848_zero_point[INT8, scalar]\\n  %856_quantized[INT8, 768x768]\\n  %856_scale[FLOAT, scalar]\\n  %856_zero_point[INT8, scalar]\\n  %857_quantized[INT8, 768x3072]\\n  %857_scale[FLOAT, scalar]\\n  %857_zero_point[INT8, scalar]\\n  %858_quantized[INT8, 3072x768]\\n  %858_scale[FLOAT, scalar]\\n  %858_zero_point[INT8, scalar]\\n  %859_quantized[INT8, 768x768]\\n  %859_scale[FLOAT, scalar]\\n  %859_zero_point[INT8, scalar]\\n  %863_quantized[INT8, 768x768]\\n  %863_scale[FLOAT, scalar]\\n  %863_zero_point[INT8, scalar]\\n  %867_quantized[INT8, 768x768]\\n  %867_scale[FLOAT, scalar]\\n  %867_zero_point[INT8, scalar]\\n  %875_quantized[INT8, 768x768]\\n  %875_scale[FLOAT, scalar]\\n  %875_zero_point[INT8, scalar]\\n  %876_quantized[INT8, 768x3072]\\n  %876_scale[FLOAT, scalar]\\n  %876_zero_point[INT8, scalar]\\n  %877_quantized[INT8, 3072x768]\\n  %877_scale[FLOAT, scalar]\\n  %877_zero_point[INT8, scalar]\\n  %878_quantized[INT8, 768x768]\\n  %878_scale[FLOAT, scalar]\\n  %878_zero_point[INT8, scalar]\\n  %882_quantized[INT8, 768x768]\\n  %882_scale[FLOAT, scalar]\\n  %882_zero_point[INT8, scalar]\\n  %886_quantized[INT8, 768x768]\\n  %886_scale[FLOAT, scalar]\\n  %886_zero_point[INT8, scalar]\\n  %894_quantized[INT8, 768x768]\\n  %894_scale[FLOAT, scalar]\\n  %894_zero_point[INT8, scalar]\\n  %895_quantized[INT8, 768x3072]\\n  %895_scale[FLOAT, scalar]\\n  %895_zero_point[INT8, scalar]\\n  %896_quantized[INT8, 3072x768]\\n  %896_scale[FLOAT, scalar]\\n  %896_zero_point[INT8, scalar]\\n  %897_quantized[INT8, 768x768]\\n  %897_scale[FLOAT, scalar]\\n  %897_zero_point[INT8, scalar]\\n  %901_quantized[INT8, 768x768]\\n  %901_scale[FLOAT, scalar]\\n  %901_zero_point[INT8, scalar]\\n  %905_quantized[INT8, 768x768]\\n  %905_scale[FLOAT, scalar]\\n  %905_zero_point[INT8, scalar]\\n  %913_quantized[INT8, 768x768]\\n  %913_scale[FLOAT, scalar]\\n  %913_zero_point[INT8, scalar]\\n  %914_quantized[INT8, 768x3072]\\n  %914_scale[FLOAT, scalar]\\n  %914_zero_point[INT8, scalar]\\n  %915_quantized[INT8, 3072x768]\\n  %915_scale[FLOAT, scalar]\\n  %915_zero_point[INT8, scalar]\\n  %916_quantized[INT8, 768x768]\\n  %916_scale[FLOAT, scalar]\\n  %916_zero_point[INT8, scalar]\\n  %920_quantized[INT8, 768x768]\\n  %920_scale[FLOAT, scalar]\\n  %920_zero_point[INT8, scalar]\\n  %924_quantized[INT8, 768x768]\\n  %924_scale[FLOAT, scalar]\\n  %924_zero_point[INT8, scalar]\\n  %932_quantized[INT8, 768x768]\\n  %932_scale[FLOAT, scalar]\\n  %932_zero_point[INT8, scalar]\\n  %933_quantized[INT8, 768x3072]\\n  %933_scale[FLOAT, scalar]\\n  %933_zero_point[INT8, scalar]\\n  %934_quantized[INT8, 3072x768]\\n  %934_scale[FLOAT, scalar]\\n  %934_zero_point[INT8, scalar]\\n  %pre_classifier.weight_quantized[INT8, 768x768]\\n  %pre_classifier.weight_scale[FLOAT, scalar]\\n  %pre_classifier.weight_zero_point[INT8, scalar]\\n  %classifier.weight_quantized[INT8, 768x2]\\n  %classifier.weight_scale[FLOAT, scalar]\\n  %classifier.weight_zero_point[INT8, scalar]\\n) {\\n  %106 = Shape(%input_ids)\\n  %107 = Constant[value = <Scalar Tensor []>]()\\n  %108 = Gather[axis = 0](%106, %107)\\n  %109 = Unsqueeze[axes = [0]](%108)\\n  %110 = ConstantOfShape[value = <Tensor>](%109)\\n  %111 = NonZero(%110)\\n  %112 = Transpose[perm = [1, 0]](%111)\\n  %113 = Squeeze[axes = [1]](%112)\\n  %114 = Cast[to = 7](%113)\\n  %115 = Unsqueeze[axes = [0]](%114)\\n  %116 = Shape(%input_ids)\\n  %117 = Expand(%115, %116)\\n  %118_quantized = Gather(%distilbert.embeddings.word_embeddings.weight_quantized, %input_ids)\\n  %119_quantized = Gather(%distilbert.embeddings.position_embeddings.weight_quantized, %117)\\n  %118 = DequantizeLinear(%118_quantized, %distilbert.embeddings.word_embeddings.weight_scale, %distilbert.embeddings.word_embeddings.weight_zero_point)\\n  %119 = DequantizeLinear(%119_quantized, %distilbert.embeddings.position_embeddings.weight_scale, %distilbert.embeddings.position_embeddings.weight_zero_point)\\n  %120 = Add(%118, %119)\\n  %121 = ReduceMean[axes = [-1]](%120)\\n  %122 = Sub(%120, %121)\\n  %123 = Constant[value = <Scalar Tensor []>]()\\n  %124 = Pow(%122, %123)\\n  %125 = ReduceMean[axes = [-1]](%124)\\n  %126 = Constant[value = <Scalar Tensor []>]()\\n  %127 = Add(%125, %126)\\n  %128 = Sqrt(%127)\\n  %129 = Div(%122, %128)\\n  %130 = Mul(%129, %distilbert.embeddings.LayerNorm.weight)\\n  %131 = Add(%130, %distilbert.embeddings.LayerNorm.bias)\\n  %132 = Shape(%131)\\n  %133 = Constant[value = <Scalar Tensor []>]()\\n  %134 = Gather[axis = 0](%132, %133)\\n  %135 = Shape(%131)\\n  %136 = Constant[value = <Scalar Tensor []>]()\\n  %137 = Gather[axis = 0](%135, %136)\\n  %131_quantized, %131_scale, %131_zero_point = DynamicQuantizeLinear(%131)\\n  %139_output_quantized = MatMulInteger(%131_quantized, %821_quantized, %131_zero_point, %821_zero_point)\\n  %139_output_quantized_cast_output = Cast[to = 1](%139_output_quantized)\\n  %MatMul_32_quant_scales_mul:0 = Mul(%131_scale, %821_scale)\\n  %139 = Mul(%139_output_quantized_cast_output, %MatMul_32_quant_scales_mul:0)\\n  %140 = Add(%139, %distilbert.transformer.layer.0.attention.q_lin.bias)\\n  %144 = Unsqueeze[axes = [0]](%134)\\n  %148 = Concat[axis = 0](%144, %822, %823, %824)\\n  %149 = Reshape(%140, %148)\\n  %150 = Transpose[perm = [0, 2, 1, 3]](%149)\\n  %152_output_quantized = MatMulInteger(%131_quantized, %825_quantized, %131_zero_point, %825_zero_point)\\n  %152_output_quantized_cast_output = Cast[to = 1](%152_output_quantized)\\n  %MatMul_38_quant_scales_mul:0 = Mul(%131_scale, %825_scale)\\n  %152 = Mul(%152_output_quantized_cast_output, %MatMul_38_quant_scales_mul:0)\\n  %153 = Add(%152, %distilbert.transformer.layer.0.attention.k_lin.bias)\\n  %157 = Unsqueeze[axes = [0]](%134)\\n  %161 = Concat[axis = 0](%157, %826, %827, %828)\\n  %162 = Reshape(%153, %161)\\n  %164_output_quantized = MatMulInteger(%131_quantized, %829_quantized, %131_zero_point, %829_zero_point)\\n  %164_output_quantized_cast_output = Cast[to = 1](%164_output_quantized)\\n  %MatMul_43_quant_scales_mul:0 = Mul(%131_scale, %829_scale)\\n  %164 = Mul(%164_output_quantized_cast_output, %MatMul_43_quant_scales_mul:0)\\n  %165 = Add(%164, %distilbert.transformer.layer.0.attention.v_lin.bias)\\n  %169 = Unsqueeze[axes = [0]](%134)\\n  %173 = Concat[axis = 0](%169, %830, %831, %832)\\n  %174 = Reshape(%165, %173)\\n  %175 = Transpose[perm = [0, 2, 1, 3]](%174)\\n  %176 = Constant[value = <Scalar Tensor []>]()\\n  %177 = Div(%150, %176)\\n  %178 = Transpose[perm = [0, 2, 3, 1]](%162)\\n  %177_quantized, %177_scale, %177_zero_point = DynamicQuantizeLinear(%177)\\n  %178_quantized, %178_scale, %178_zero_point = DynamicQuantizeLinear(%178)\\n  %179_output_quantized = MatMulInteger(%177_quantized, %178_quantized, %177_zero_point, %178_zero_point)\\n  %179_output_quantized_cast_output = Cast[to = 1](%179_output_quantized)\\n  %MatMul_52_quant_scales_mul:0 = Mul(%177_scale, %178_scale)\\n  %179 = Mul(%179_output_quantized_cast_output, %MatMul_52_quant_scales_mul:0)\\n  %180 = Constant[value = <Scalar Tensor []>]()\\n  %181 = Equal(%attention_mask, %180)\\n  %184 = Unsqueeze[axes = [0]](%134)\\n  %187 = Unsqueeze[axes = [0]](%137)\\n  %188 = Concat[axis = 0](%184, %833, %834, %187)\\n  %189 = Reshape(%181, %188)\\n  %190 = Shape(%179)\\n  %191 = Expand(%189, %190)\\n  %192 = Cast[to = 9](%191)\\n  %193 = Constant[value = <Scalar Tensor []>]()\\n  %194 = Where(%192, %193, %179)\\n  %195 = Softmax[axis = 3](%194)\\n  %195_quantized, %195_scale, %195_zero_point = DynamicQuantizeLinear(%195)\\n  %175_quantized, %175_scale, %175_zero_point = DynamicQuantizeLinear(%175)\\n  %196_output_quantized = MatMulInteger(%195_quantized, %175_quantized, %195_zero_point, %175_zero_point)\\n  %196_output_quantized_cast_output = Cast[to = 1](%196_output_quantized)\\n  %MatMul_65_quant_scales_mul:0 = Mul(%195_scale, %175_scale)\\n  %196 = Mul(%196_output_quantized_cast_output, %MatMul_65_quant_scales_mul:0)\\n  %197 = Transpose[perm = [0, 2, 1, 3]](%196)\\n  %200 = Unsqueeze[axes = [0]](%134)\\n  %203 = Concat[axis = 0](%200, %835, %836)\\n  %204 = Reshape(%197, %203)\\n  %204_quantized, %204_scale, %204_zero_point = DynamicQuantizeLinear(%204)\\n  %206_output_quantized = MatMulInteger(%204_quantized, %837_quantized, %204_zero_point, %837_zero_point)\\n  %206_output_quantized_cast_output = Cast[to = 1](%206_output_quantized)\\n  %MatMul_70_quant_scales_mul:0 = Mul(%204_scale, %837_scale)\\n  %206 = Mul(%206_output_quantized_cast_output, %MatMul_70_quant_scales_mul:0)\\n  %207 = Add(%206, %distilbert.transformer.layer.0.attention.out_lin.bias)\\n  %208 = Add(%207, %131)\\n  %209 = ReduceMean[axes = [-1]](%208)\\n  %210 = Sub(%208, %209)\\n  %211 = Constant[value = <Scalar Tensor []>]()\\n  %212 = Pow(%210, %211)\\n  %213 = ReduceMean[axes = [-1]](%212)\\n  %214 = Constant[value = <Scalar Tensor []>]()\\n  %215 = Add(%213, %214)\\n  %216 = Sqrt(%215)\\n  %217 = Div(%210, %216)\\n  %218 = Mul(%217, %distilbert.transformer.layer.0.sa_layer_norm.weight)\\n  %219 = Add(%218, %distilbert.transformer.layer.0.sa_layer_norm.bias)\\n  %219_quantized, %219_scale, %219_zero_point = DynamicQuantizeLinear(%219)\\n  %221_output_quantized = MatMulInteger(%219_quantized, %838_quantized, %219_zero_point, %838_zero_point)\\n  %221_output_quantized_cast_output = Cast[to = 1](%221_output_quantized)\\n  %MatMul_84_quant_scales_mul:0 = Mul(%219_scale, %838_scale)\\n  %221 = Mul(%221_output_quantized_cast_output, %MatMul_84_quant_scales_mul:0)\\n  %222 = Add(%221, %distilbert.transformer.layer.0.ffn.lin1.bias)\\n  %223 = Constant[value = <Scalar Tensor []>]()\\n  %224 = Div(%222, %223)\\n  %225 = Erf(%224)\\n  %226 = Constant[value = <Scalar Tensor []>]()\\n  %227 = Add(%225, %226)\\n  %228 = Mul(%222, %227)\\n  %229 = Constant[value = <Scalar Tensor []>]()\\n  %230 = Mul(%228, %229)\\n  %230_quantized, %230_scale, %230_zero_point = DynamicQuantizeLinear(%230)\\n  %232_output_quantized = MatMulInteger(%230_quantized, %839_quantized, %230_zero_point, %839_zero_point)\\n  %232_output_quantized_cast_output = Cast[to = 1](%232_output_quantized)\\n  %MatMul_94_quant_scales_mul:0 = Mul(%230_scale, %839_scale)\\n  %232 = Mul(%232_output_quantized_cast_output, %MatMul_94_quant_scales_mul:0)\\n  %233 = Add(%232, %distilbert.transformer.layer.0.ffn.lin2.bias)\\n  %234 = Add(%233, %219)\\n  %235 = ReduceMean[axes = [-1]](%234)\\n  %236 = Sub(%234, %235)\\n  %237 = Constant[value = <Scalar Tensor []>]()\\n  %238 = Pow(%236, %237)\\n  %239 = ReduceMean[axes = [-1]](%238)\\n  %240 = Constant[value = <Scalar Tensor []>]()\\n  %241 = Add(%239, %240)\\n  %242 = Sqrt(%241)\\n  %243 = Div(%236, %242)\\n  %244 = Mul(%243, %distilbert.transformer.layer.0.output_layer_norm.weight)\\n  %245 = Add(%244, %distilbert.transformer.layer.0.output_layer_norm.bias)\\n  %246 = Shape(%245)\\n  %247 = Constant[value = <Scalar Tensor []>]()\\n  %248 = Gather[axis = 0](%246, %247)\\n  %249 = Shape(%245)\\n  %250 = Constant[value = <Scalar Tensor []>]()\\n  %251 = Gather[axis = 0](%249, %250)\\n  %245_quantized, %245_scale, %245_zero_point = DynamicQuantizeLinear(%245)\\n  %253_output_quantized = MatMulInteger(%245_quantized, %840_quantized, %245_zero_point, %840_zero_point)\\n  %253_output_quantized_cast_output = Cast[to = 1](%253_output_quantized)\\n  %MatMul_114_quant_scales_mul:0 = Mul(%245_scale, %840_scale)\\n  %253 = Mul(%253_output_quantized_cast_output, %MatMul_114_quant_scales_mul:0)\\n  %254 = Add(%253, %distilbert.transformer.layer.1.attention.q_lin.bias)\\n  %258 = Unsqueeze[axes = [0]](%248)\\n  %262 = Concat[axis = 0](%258, %841, %842, %843)\\n  %263 = Reshape(%254, %262)\\n  %264 = Transpose[perm = [0, 2, 1, 3]](%263)\\n  %266_output_quantized = MatMulInteger(%245_quantized, %844_quantized, %245_zero_point, %844_zero_point)\\n  %266_output_quantized_cast_output = Cast[to = 1](%266_output_quantized)\\n  %MatMul_120_quant_scales_mul:0 = Mul(%245_scale, %844_scale)\\n  %266 = Mul(%266_output_quantized_cast_output, %MatMul_120_quant_scales_mul:0)\\n  %267 = Add(%266, %distilbert.transformer.layer.1.attention.k_lin.bias)\\n  %271 = Unsqueeze[axes = [0]](%248)\\n  %275 = Concat[axis = 0](%271, %845, %846, %847)\\n  %276 = Reshape(%267, %275)\\n  %278_output_quantized = MatMulInteger(%245_quantized, %848_quantized, %245_zero_point, %848_zero_point)\\n  %278_output_quantized_cast_output = Cast[to = 1](%278_output_quantized)\\n  %MatMul_125_quant_scales_mul:0 = Mul(%245_scale, %848_scale)\\n  %278 = Mul(%278_output_quantized_cast_output, %MatMul_125_quant_scales_mul:0)\\n  %279 = Add(%278, %distilbert.transformer.layer.1.attention.v_lin.bias)\\n  %283 = Unsqueeze[axes = [0]](%248)\\n  %287 = Concat[axis = 0](%283, %849, %850, %851)\\n  %288 = Reshape(%279, %287)\\n  %289 = Transpose[perm = [0, 2, 1, 3]](%288)\\n  %290 = Constant[value = <Scalar Tensor []>]()\\n  %291 = Div(%264, %290)\\n  %292 = Transpose[perm = [0, 2, 3, 1]](%276)\\n  %291_quantized, %291_scale, %291_zero_point = DynamicQuantizeLinear(%291)\\n  %292_quantized, %292_scale, %292_zero_point = DynamicQuantizeLinear(%292)\\n  %293_output_quantized = MatMulInteger(%291_quantized, %292_quantized, %291_zero_point, %292_zero_point)\\n  %293_output_quantized_cast_output = Cast[to = 1](%293_output_quantized)\\n  %MatMul_134_quant_scales_mul:0 = Mul(%291_scale, %292_scale)\\n  %293 = Mul(%293_output_quantized_cast_output, %MatMul_134_quant_scales_mul:0)\\n  %294 = Constant[value = <Scalar Tensor []>]()\\n  %295 = Equal(%attention_mask, %294)\\n  %298 = Unsqueeze[axes = [0]](%248)\\n  %301 = Unsqueeze[axes = [0]](%251)\\n  %302 = Concat[axis = 0](%298, %852, %853, %301)\\n  %303 = Reshape(%295, %302)\\n  %304 = Shape(%293)\\n  %305 = Expand(%303, %304)\\n  %306 = Cast[to = 9](%305)\\n  %307 = Constant[value = <Scalar Tensor []>]()\\n  %308 = Where(%306, %307, %293)\\n  %309 = Softmax[axis = 3](%308)\\n  %309_quantized, %309_scale, %309_zero_point = DynamicQuantizeLinear(%309)\\n  %289_quantized, %289_scale, %289_zero_point = DynamicQuantizeLinear(%289)\\n  %310_output_quantized = MatMulInteger(%309_quantized, %289_quantized, %309_zero_point, %289_zero_point)\\n  %310_output_quantized_cast_output = Cast[to = 1](%310_output_quantized)\\n  %MatMul_147_quant_scales_mul:0 = Mul(%309_scale, %289_scale)\\n  %310 = Mul(%310_output_quantized_cast_output, %MatMul_147_quant_scales_mul:0)\\n  %311 = Transpose[perm = [0, 2, 1, 3]](%310)\\n  %314 = Unsqueeze[axes = [0]](%248)\\n  %317 = Concat[axis = 0](%314, %854, %855)\\n  %318 = Reshape(%311, %317)\\n  %318_quantized, %318_scale, %318_zero_point = DynamicQuantizeLinear(%318)\\n  %320_output_quantized = MatMulInteger(%318_quantized, %856_quantized, %318_zero_point, %856_zero_point)\\n  %320_output_quantized_cast_output = Cast[to = 1](%320_output_quantized)\\n  %MatMul_152_quant_scales_mul:0 = Mul(%318_scale, %856_scale)\\n  %320 = Mul(%320_output_quantized_cast_output, %MatMul_152_quant_scales_mul:0)\\n  %321 = Add(%320, %distilbert.transformer.layer.1.attention.out_lin.bias)\\n  %322 = Add(%321, %245)\\n  %323 = ReduceMean[axes = [-1]](%322)\\n  %324 = Sub(%322, %323)\\n  %325 = Constant[value = <Scalar Tensor []>]()\\n  %326 = Pow(%324, %325)\\n  %327 = ReduceMean[axes = [-1]](%326)\\n  %328 = Constant[value = <Scalar Tensor []>]()\\n  %329 = Add(%327, %328)\\n  %330 = Sqrt(%329)\\n  %331 = Div(%324, %330)\\n  %332 = Mul(%331, %distilbert.transformer.layer.1.sa_layer_norm.weight)\\n  %333 = Add(%332, %distilbert.transformer.layer.1.sa_layer_norm.bias)\\n  %333_quantized, %333_scale, %333_zero_point = DynamicQuantizeLinear(%333)\\n  %335_output_quantized = MatMulInteger(%333_quantized, %857_quantized, %333_zero_point, %857_zero_point)\\n  %335_output_quantized_cast_output = Cast[to = 1](%335_output_quantized)\\n  %MatMul_166_quant_scales_mul:0 = Mul(%333_scale, %857_scale)\\n  %335 = Mul(%335_output_quantized_cast_output, %MatMul_166_quant_scales_mul:0)\\n  %336 = Add(%335, %distilbert.transformer.layer.1.ffn.lin1.bias)\\n  %337 = Constant[value = <Scalar Tensor []>]()\\n  %338 = Div(%336, %337)\\n  %339 = Erf(%338)\\n  %340 = Constant[value = <Scalar Tensor []>]()\\n  %341 = Add(%339, %340)\\n  %342 = Mul(%336, %341)\\n  %343 = Constant[value = <Scalar Tensor []>]()\\n  %344 = Mul(%342, %343)\\n  %344_quantized, %344_scale, %344_zero_point = DynamicQuantizeLinear(%344)\\n  %346_output_quantized = MatMulInteger(%344_quantized, %858_quantized, %344_zero_point, %858_zero_point)\\n  %346_output_quantized_cast_output = Cast[to = 1](%346_output_quantized)\\n  %MatMul_176_quant_scales_mul:0 = Mul(%344_scale, %858_scale)\\n  %346 = Mul(%346_output_quantized_cast_output, %MatMul_176_quant_scales_mul:0)\\n  %347 = Add(%346, %distilbert.transformer.layer.1.ffn.lin2.bias)\\n  %348 = Add(%347, %333)\\n  %349 = ReduceMean[axes = [-1]](%348)\\n  %350 = Sub(%348, %349)\\n  %351 = Constant[value = <Scalar Tensor []>]()\\n  %352 = Pow(%350, %351)\\n  %353 = ReduceMean[axes = [-1]](%352)\\n  %354 = Constant[value = <Scalar Tensor []>]()\\n  %355 = Add(%353, %354)\\n  %356 = Sqrt(%355)\\n  %357 = Div(%350, %356)\\n  %358 = Mul(%357, %distilbert.transformer.layer.1.output_layer_norm.weight)\\n  %359 = Add(%358, %distilbert.transformer.layer.1.output_layer_norm.bias)\\n  %360 = Shape(%359)\\n  %361 = Constant[value = <Scalar Tensor []>]()\\n  %362 = Gather[axis = 0](%360, %361)\\n  %363 = Shape(%359)\\n  %364 = Constant[value = <Scalar Tensor []>]()\\n  %365 = Gather[axis = 0](%363, %364)\\n  %359_quantized, %359_scale, %359_zero_point = DynamicQuantizeLinear(%359)\\n  %367_output_quantized = MatMulInteger(%359_quantized, %859_quantized, %359_zero_point, %859_zero_point)\\n  %367_output_quantized_cast_output = Cast[to = 1](%367_output_quantized)\\n  %MatMul_196_quant_scales_mul:0 = Mul(%359_scale, %859_scale)\\n  %367 = Mul(%367_output_quantized_cast_output, %MatMul_196_quant_scales_mul:0)\\n  %368 = Add(%367, %distilbert.transformer.layer.2.attention.q_lin.bias)\\n  %372 = Unsqueeze[axes = [0]](%362)\\n  %376 = Concat[axis = 0](%372, %860, %861, %862)\\n  %377 = Reshape(%368, %376)\\n  %378 = Transpose[perm = [0, 2, 1, 3]](%377)\\n  %380_output_quantized = MatMulInteger(%359_quantized, %863_quantized, %359_zero_point, %863_zero_point)\\n  %380_output_quantized_cast_output = Cast[to = 1](%380_output_quantized)\\n  %MatMul_202_quant_scales_mul:0 = Mul(%359_scale, %863_scale)\\n  %380 = Mul(%380_output_quantized_cast_output, %MatMul_202_quant_scales_mul:0)\\n  %381 = Add(%380, %distilbert.transformer.layer.2.attention.k_lin.bias)\\n  %385 = Unsqueeze[axes = [0]](%362)\\n  %389 = Concat[axis = 0](%385, %864, %865, %866)\\n  %390 = Reshape(%381, %389)\\n  %392_output_quantized = MatMulInteger(%359_quantized, %867_quantized, %359_zero_point, %867_zero_point)\\n  %392_output_quantized_cast_output = Cast[to = 1](%392_output_quantized)\\n  %MatMul_207_quant_scales_mul:0 = Mul(%359_scale, %867_scale)\\n  %392 = Mul(%392_output_quantized_cast_output, %MatMul_207_quant_scales_mul:0)\\n  %393 = Add(%392, %distilbert.transformer.layer.2.attention.v_lin.bias)\\n  %397 = Unsqueeze[axes = [0]](%362)\\n  %401 = Concat[axis = 0](%397, %868, %869, %870)\\n  %402 = Reshape(%393, %401)\\n  %403 = Transpose[perm = [0, 2, 1, 3]](%402)\\n  %404 = Constant[value = <Scalar Tensor []>]()\\n  %405 = Div(%378, %404)\\n  %406 = Transpose[perm = [0, 2, 3, 1]](%390)\\n  %405_quantized, %405_scale, %405_zero_point = DynamicQuantizeLinear(%405)\\n  %406_quantized, %406_scale, %406_zero_point = DynamicQuantizeLinear(%406)\\n  %407_output_quantized = MatMulInteger(%405_quantized, %406_quantized, %405_zero_point, %406_zero_point)\\n  %407_output_quantized_cast_output = Cast[to = 1](%407_output_quantized)\\n  %MatMul_216_quant_scales_mul:0 = Mul(%405_scale, %406_scale)\\n  %407 = Mul(%407_output_quantized_cast_output, %MatMul_216_quant_scales_mul:0)\\n  %408 = Constant[value = <Scalar Tensor []>]()\\n  %409 = Equal(%attention_mask, %408)\\n  %412 = Unsqueeze[axes = [0]](%362)\\n  %415 = Unsqueeze[axes = [0]](%365)\\n  %416 = Concat[axis = 0](%412, %871, %872, %415)\\n  %417 = Reshape(%409, %416)\\n  %418 = Shape(%407)\\n  %419 = Expand(%417, %418)\\n  %420 = Cast[to = 9](%419)\\n  %421 = Constant[value = <Scalar Tensor []>]()\\n  %422 = Where(%420, %421, %407)\\n  %423 = Softmax[axis = 3](%422)\\n  %423_quantized, %423_scale, %423_zero_point = DynamicQuantizeLinear(%423)\\n  %403_quantized, %403_scale, %403_zero_point = DynamicQuantizeLinear(%403)\\n  %424_output_quantized = MatMulInteger(%423_quantized, %403_quantized, %423_zero_point, %403_zero_point)\\n  %424_output_quantized_cast_output = Cast[to = 1](%424_output_quantized)\\n  %MatMul_229_quant_scales_mul:0 = Mul(%423_scale, %403_scale)\\n  %424 = Mul(%424_output_quantized_cast_output, %MatMul_229_quant_scales_mul:0)\\n  %425 = Transpose[perm = [0, 2, 1, 3]](%424)\\n  %428 = Unsqueeze[axes = [0]](%362)\\n  %431 = Concat[axis = 0](%428, %873, %874)\\n  %432 = Reshape(%425, %431)\\n  %432_quantized, %432_scale, %432_zero_point = DynamicQuantizeLinear(%432)\\n  %434_output_quantized = MatMulInteger(%432_quantized, %875_quantized, %432_zero_point, %875_zero_point)\\n  %434_output_quantized_cast_output = Cast[to = 1](%434_output_quantized)\\n  %MatMul_234_quant_scales_mul:0 = Mul(%432_scale, %875_scale)\\n  %434 = Mul(%434_output_quantized_cast_output, %MatMul_234_quant_scales_mul:0)\\n  %435 = Add(%434, %distilbert.transformer.layer.2.attention.out_lin.bias)\\n  %436 = Add(%435, %359)\\n  %437 = ReduceMean[axes = [-1]](%436)\\n  %438 = Sub(%436, %437)\\n  %439 = Constant[value = <Scalar Tensor []>]()\\n  %440 = Pow(%438, %439)\\n  %441 = ReduceMean[axes = [-1]](%440)\\n  %442 = Constant[value = <Scalar Tensor []>]()\\n  %443 = Add(%441, %442)\\n  %444 = Sqrt(%443)\\n  %445 = Div(%438, %444)\\n  %446 = Mul(%445, %distilbert.transformer.layer.2.sa_layer_norm.weight)\\n  %447 = Add(%446, %distilbert.transformer.layer.2.sa_layer_norm.bias)\\n  %447_quantized, %447_scale, %447_zero_point = DynamicQuantizeLinear(%447)\\n  %449_output_quantized = MatMulInteger(%447_quantized, %876_quantized, %447_zero_point, %876_zero_point)\\n  %449_output_quantized_cast_output = Cast[to = 1](%449_output_quantized)\\n  %MatMul_248_quant_scales_mul:0 = Mul(%447_scale, %876_scale)\\n  %449 = Mul(%449_output_quantized_cast_output, %MatMul_248_quant_scales_mul:0)\\n  %450 = Add(%449, %distilbert.transformer.layer.2.ffn.lin1.bias)\\n  %451 = Constant[value = <Scalar Tensor []>]()\\n  %452 = Div(%450, %451)\\n  %453 = Erf(%452)\\n  %454 = Constant[value = <Scalar Tensor []>]()\\n  %455 = Add(%453, %454)\\n  %456 = Mul(%450, %455)\\n  %457 = Constant[value = <Scalar Tensor []>]()\\n  %458 = Mul(%456, %457)\\n  %458_quantized, %458_scale, %458_zero_point = DynamicQuantizeLinear(%458)\\n  %460_output_quantized = MatMulInteger(%458_quantized, %877_quantized, %458_zero_point, %877_zero_point)\\n  %460_output_quantized_cast_output = Cast[to = 1](%460_output_quantized)\\n  %MatMul_258_quant_scales_mul:0 = Mul(%458_scale, %877_scale)\\n  %460 = Mul(%460_output_quantized_cast_output, %MatMul_258_quant_scales_mul:0)\\n  %461 = Add(%460, %distilbert.transformer.layer.2.ffn.lin2.bias)\\n  %462 = Add(%461, %447)\\n  %463 = ReduceMean[axes = [-1]](%462)\\n  %464 = Sub(%462, %463)\\n  %465 = Constant[value = <Scalar Tensor []>]()\\n  %466 = Pow(%464, %465)\\n  %467 = ReduceMean[axes = [-1]](%466)\\n  %468 = Constant[value = <Scalar Tensor []>]()\\n  %469 = Add(%467, %468)\\n  %470 = Sqrt(%469)\\n  %471 = Div(%464, %470)\\n  %472 = Mul(%471, %distilbert.transformer.layer.2.output_layer_norm.weight)\\n  %473 = Add(%472, %distilbert.transformer.layer.2.output_layer_norm.bias)\\n  %474 = Shape(%473)\\n  %475 = Constant[value = <Scalar Tensor []>]()\\n  %476 = Gather[axis = 0](%474, %475)\\n  %477 = Shape(%473)\\n  %478 = Constant[value = <Scalar Tensor []>]()\\n  %479 = Gather[axis = 0](%477, %478)\\n  %473_quantized, %473_scale, %473_zero_point = DynamicQuantizeLinear(%473)\\n  %481_output_quantized = MatMulInteger(%473_quantized, %878_quantized, %473_zero_point, %878_zero_point)\\n  %481_output_quantized_cast_output = Cast[to = 1](%481_output_quantized)\\n  %MatMul_278_quant_scales_mul:0 = Mul(%473_scale, %878_scale)\\n  %481 = Mul(%481_output_quantized_cast_output, %MatMul_278_quant_scales_mul:0)\\n  %482 = Add(%481, %distilbert.transformer.layer.3.attention.q_lin.bias)\\n  %486 = Unsqueeze[axes = [0]](%476)\\n  %490 = Concat[axis = 0](%486, %879, %880, %881)\\n  %491 = Reshape(%482, %490)\\n  %492 = Transpose[perm = [0, 2, 1, 3]](%491)\\n  %494_output_quantized = MatMulInteger(%473_quantized, %882_quantized, %473_zero_point, %882_zero_point)\\n  %494_output_quantized_cast_output = Cast[to = 1](%494_output_quantized)\\n  %MatMul_284_quant_scales_mul:0 = Mul(%473_scale, %882_scale)\\n  %494 = Mul(%494_output_quantized_cast_output, %MatMul_284_quant_scales_mul:0)\\n  %495 = Add(%494, %distilbert.transformer.layer.3.attention.k_lin.bias)\\n  %499 = Unsqueeze[axes = [0]](%476)\\n  %503 = Concat[axis = 0](%499, %883, %884, %885)\\n  %504 = Reshape(%495, %503)\\n  %506_output_quantized = MatMulInteger(%473_quantized, %886_quantized, %473_zero_point, %886_zero_point)\\n  %506_output_quantized_cast_output = Cast[to = 1](%506_output_quantized)\\n  %MatMul_289_quant_scales_mul:0 = Mul(%473_scale, %886_scale)\\n  %506 = Mul(%506_output_quantized_cast_output, %MatMul_289_quant_scales_mul:0)\\n  %507 = Add(%506, %distilbert.transformer.layer.3.attention.v_lin.bias)\\n  %511 = Unsqueeze[axes = [0]](%476)\\n  %515 = Concat[axis = 0](%511, %887, %888, %889)\\n  %516 = Reshape(%507, %515)\\n  %517 = Transpose[perm = [0, 2, 1, 3]](%516)\\n  %518 = Constant[value = <Scalar Tensor []>]()\\n  %519 = Div(%492, %518)\\n  %520 = Transpose[perm = [0, 2, 3, 1]](%504)\\n  %519_quantized, %519_scale, %519_zero_point = DynamicQuantizeLinear(%519)\\n  %520_quantized, %520_scale, %520_zero_point = DynamicQuantizeLinear(%520)\\n  %521_output_quantized = MatMulInteger(%519_quantized, %520_quantized, %519_zero_point, %520_zero_point)\\n  %521_output_quantized_cast_output = Cast[to = 1](%521_output_quantized)\\n  %MatMul_298_quant_scales_mul:0 = Mul(%519_scale, %520_scale)\\n  %521 = Mul(%521_output_quantized_cast_output, %MatMul_298_quant_scales_mul:0)\\n  %522 = Constant[value = <Scalar Tensor []>]()\\n  %523 = Equal(%attention_mask, %522)\\n  %526 = Unsqueeze[axes = [0]](%476)\\n  %529 = Unsqueeze[axes = [0]](%479)\\n  %530 = Concat[axis = 0](%526, %890, %891, %529)\\n  %531 = Reshape(%523, %530)\\n  %532 = Shape(%521)\\n  %533 = Expand(%531, %532)\\n  %534 = Cast[to = 9](%533)\\n  %535 = Constant[value = <Scalar Tensor []>]()\\n  %536 = Where(%534, %535, %521)\\n  %537 = Softmax[axis = 3](%536)\\n  %537_quantized, %537_scale, %537_zero_point = DynamicQuantizeLinear(%537)\\n  %517_quantized, %517_scale, %517_zero_point = DynamicQuantizeLinear(%517)\\n  %538_output_quantized = MatMulInteger(%537_quantized, %517_quantized, %537_zero_point, %517_zero_point)\\n  %538_output_quantized_cast_output = Cast[to = 1](%538_output_quantized)\\n  %MatMul_311_quant_scales_mul:0 = Mul(%537_scale, %517_scale)\\n  %538 = Mul(%538_output_quantized_cast_output, %MatMul_311_quant_scales_mul:0)\\n  %539 = Transpose[perm = [0, 2, 1, 3]](%538)\\n  %542 = Unsqueeze[axes = [0]](%476)\\n  %545 = Concat[axis = 0](%542, %892, %893)\\n  %546 = Reshape(%539, %545)\\n  %546_quantized, %546_scale, %546_zero_point = DynamicQuantizeLinear(%546)\\n  %548_output_quantized = MatMulInteger(%546_quantized, %894_quantized, %546_zero_point, %894_zero_point)\\n  %548_output_quantized_cast_output = Cast[to = 1](%548_output_quantized)\\n  %MatMul_316_quant_scales_mul:0 = Mul(%546_scale, %894_scale)\\n  %548 = Mul(%548_output_quantized_cast_output, %MatMul_316_quant_scales_mul:0)\\n  %549 = Add(%548, %distilbert.transformer.layer.3.attention.out_lin.bias)\\n  %550 = Add(%549, %473)\\n  %551 = ReduceMean[axes = [-1]](%550)\\n  %552 = Sub(%550, %551)\\n  %553 = Constant[value = <Scalar Tensor []>]()\\n  %554 = Pow(%552, %553)\\n  %555 = ReduceMean[axes = [-1]](%554)\\n  %556 = Constant[value = <Scalar Tensor []>]()\\n  %557 = Add(%555, %556)\\n  %558 = Sqrt(%557)\\n  %559 = Div(%552, %558)\\n  %560 = Mul(%559, %distilbert.transformer.layer.3.sa_layer_norm.weight)\\n  %561 = Add(%560, %distilbert.transformer.layer.3.sa_layer_norm.bias)\\n  %561_quantized, %561_scale, %561_zero_point = DynamicQuantizeLinear(%561)\\n  %563_output_quantized = MatMulInteger(%561_quantized, %895_quantized, %561_zero_point, %895_zero_point)\\n  %563_output_quantized_cast_output = Cast[to = 1](%563_output_quantized)\\n  %MatMul_330_quant_scales_mul:0 = Mul(%561_scale, %895_scale)\\n  %563 = Mul(%563_output_quantized_cast_output, %MatMul_330_quant_scales_mul:0)\\n  %564 = Add(%563, %distilbert.transformer.layer.3.ffn.lin1.bias)\\n  %565 = Constant[value = <Scalar Tensor []>]()\\n  %566 = Div(%564, %565)\\n  %567 = Erf(%566)\\n  %568 = Constant[value = <Scalar Tensor []>]()\\n  %569 = Add(%567, %568)\\n  %570 = Mul(%564, %569)\\n  %571 = Constant[value = <Scalar Tensor []>]()\\n  %572 = Mul(%570, %571)\\n  %572_quantized, %572_scale, %572_zero_point = DynamicQuantizeLinear(%572)\\n  %574_output_quantized = MatMulInteger(%572_quantized, %896_quantized, %572_zero_point, %896_zero_point)\\n  %574_output_quantized_cast_output = Cast[to = 1](%574_output_quantized)\\n  %MatMul_340_quant_scales_mul:0 = Mul(%572_scale, %896_scale)\\n  %574 = Mul(%574_output_quantized_cast_output, %MatMul_340_quant_scales_mul:0)\\n  %575 = Add(%574, %distilbert.transformer.layer.3.ffn.lin2.bias)\\n  %576 = Add(%575, %561)\\n  %577 = ReduceMean[axes = [-1]](%576)\\n  %578 = Sub(%576, %577)\\n  %579 = Constant[value = <Scalar Tensor []>]()\\n  %580 = Pow(%578, %579)\\n  %581 = ReduceMean[axes = [-1]](%580)\\n  %582 = Constant[value = <Scalar Tensor []>]()\\n  %583 = Add(%581, %582)\\n  %584 = Sqrt(%583)\\n  %585 = Div(%578, %584)\\n  %586 = Mul(%585, %distilbert.transformer.layer.3.output_layer_norm.weight)\\n  %587 = Add(%586, %distilbert.transformer.layer.3.output_layer_norm.bias)\\n  %588 = Shape(%587)\\n  %589 = Constant[value = <Scalar Tensor []>]()\\n  %590 = Gather[axis = 0](%588, %589)\\n  %591 = Shape(%587)\\n  %592 = Constant[value = <Scalar Tensor []>]()\\n  %593 = Gather[axis = 0](%591, %592)\\n  %587_quantized, %587_scale, %587_zero_point = DynamicQuantizeLinear(%587)\\n  %595_output_quantized = MatMulInteger(%587_quantized, %897_quantized, %587_zero_point, %897_zero_point)\\n  %595_output_quantized_cast_output = Cast[to = 1](%595_output_quantized)\\n  %MatMul_360_quant_scales_mul:0 = Mul(%587_scale, %897_scale)\\n  %595 = Mul(%595_output_quantized_cast_output, %MatMul_360_quant_scales_mul:0)\\n  %596 = Add(%595, %distilbert.transformer.layer.4.attention.q_lin.bias)\\n  %600 = Unsqueeze[axes = [0]](%590)\\n  %604 = Concat[axis = 0](%600, %898, %899, %900)\\n  %605 = Reshape(%596, %604)\\n  %606 = Transpose[perm = [0, 2, 1, 3]](%605)\\n  %608_output_quantized = MatMulInteger(%587_quantized, %901_quantized, %587_zero_point, %901_zero_point)\\n  %608_output_quantized_cast_output = Cast[to = 1](%608_output_quantized)\\n  %MatMul_366_quant_scales_mul:0 = Mul(%587_scale, %901_scale)\\n  %608 = Mul(%608_output_quantized_cast_output, %MatMul_366_quant_scales_mul:0)\\n  %609 = Add(%608, %distilbert.transformer.layer.4.attention.k_lin.bias)\\n  %613 = Unsqueeze[axes = [0]](%590)\\n  %617 = Concat[axis = 0](%613, %902, %903, %904)\\n  %618 = Reshape(%609, %617)\\n  %620_output_quantized = MatMulInteger(%587_quantized, %905_quantized, %587_zero_point, %905_zero_point)\\n  %620_output_quantized_cast_output = Cast[to = 1](%620_output_quantized)\\n  %MatMul_371_quant_scales_mul:0 = Mul(%587_scale, %905_scale)\\n  %620 = Mul(%620_output_quantized_cast_output, %MatMul_371_quant_scales_mul:0)\\n  %621 = Add(%620, %distilbert.transformer.layer.4.attention.v_lin.bias)\\n  %625 = Unsqueeze[axes = [0]](%590)\\n  %629 = Concat[axis = 0](%625, %906, %907, %908)\\n  %630 = Reshape(%621, %629)\\n  %631 = Transpose[perm = [0, 2, 1, 3]](%630)\\n  %632 = Constant[value = <Scalar Tensor []>]()\\n  %633 = Div(%606, %632)\\n  %634 = Transpose[perm = [0, 2, 3, 1]](%618)\\n  %633_quantized, %633_scale, %633_zero_point = DynamicQuantizeLinear(%633)\\n  %634_quantized, %634_scale, %634_zero_point = DynamicQuantizeLinear(%634)\\n  %635_output_quantized = MatMulInteger(%633_quantized, %634_quantized, %633_zero_point, %634_zero_point)\\n  %635_output_quantized_cast_output = Cast[to = 1](%635_output_quantized)\\n  %MatMul_380_quant_scales_mul:0 = Mul(%633_scale, %634_scale)\\n  %635 = Mul(%635_output_quantized_cast_output, %MatMul_380_quant_scales_mul:0)\\n  %636 = Constant[value = <Scalar Tensor []>]()\\n  %637 = Equal(%attention_mask, %636)\\n  %640 = Unsqueeze[axes = [0]](%590)\\n  %643 = Unsqueeze[axes = [0]](%593)\\n  %644 = Concat[axis = 0](%640, %909, %910, %643)\\n  %645 = Reshape(%637, %644)\\n  %646 = Shape(%635)\\n  %647 = Expand(%645, %646)\\n  %648 = Cast[to = 9](%647)\\n  %649 = Constant[value = <Scalar Tensor []>]()\\n  %650 = Where(%648, %649, %635)\\n  %651 = Softmax[axis = 3](%650)\\n  %651_quantized, %651_scale, %651_zero_point = DynamicQuantizeLinear(%651)\\n  %631_quantized, %631_scale, %631_zero_point = DynamicQuantizeLinear(%631)\\n  %652_output_quantized = MatMulInteger(%651_quantized, %631_quantized, %651_zero_point, %631_zero_point)\\n  %652_output_quantized_cast_output = Cast[to = 1](%652_output_quantized)\\n  %MatMul_393_quant_scales_mul:0 = Mul(%651_scale, %631_scale)\\n  %652 = Mul(%652_output_quantized_cast_output, %MatMul_393_quant_scales_mul:0)\\n  %653 = Transpose[perm = [0, 2, 1, 3]](%652)\\n  %656 = Unsqueeze[axes = [0]](%590)\\n  %659 = Concat[axis = 0](%656, %911, %912)\\n  %660 = Reshape(%653, %659)\\n  %660_quantized, %660_scale, %660_zero_point = DynamicQuantizeLinear(%660)\\n  %662_output_quantized = MatMulInteger(%660_quantized, %913_quantized, %660_zero_point, %913_zero_point)\\n  %662_output_quantized_cast_output = Cast[to = 1](%662_output_quantized)\\n  %MatMul_398_quant_scales_mul:0 = Mul(%660_scale, %913_scale)\\n  %662 = Mul(%662_output_quantized_cast_output, %MatMul_398_quant_scales_mul:0)\\n  %663 = Add(%662, %distilbert.transformer.layer.4.attention.out_lin.bias)\\n  %664 = Add(%663, %587)\\n  %665 = ReduceMean[axes = [-1]](%664)\\n  %666 = Sub(%664, %665)\\n  %667 = Constant[value = <Scalar Tensor []>]()\\n  %668 = Pow(%666, %667)\\n  %669 = ReduceMean[axes = [-1]](%668)\\n  %670 = Constant[value = <Scalar Tensor []>]()\\n  %671 = Add(%669, %670)\\n  %672 = Sqrt(%671)\\n  %673 = Div(%666, %672)\\n  %674 = Mul(%673, %distilbert.transformer.layer.4.sa_layer_norm.weight)\\n  %675 = Add(%674, %distilbert.transformer.layer.4.sa_layer_norm.bias)\\n  %675_quantized, %675_scale, %675_zero_point = DynamicQuantizeLinear(%675)\\n  %677_output_quantized = MatMulInteger(%675_quantized, %914_quantized, %675_zero_point, %914_zero_point)\\n  %677_output_quantized_cast_output = Cast[to = 1](%677_output_quantized)\\n  %MatMul_412_quant_scales_mul:0 = Mul(%675_scale, %914_scale)\\n  %677 = Mul(%677_output_quantized_cast_output, %MatMul_412_quant_scales_mul:0)\\n  %678 = Add(%677, %distilbert.transformer.layer.4.ffn.lin1.bias)\\n  %679 = Constant[value = <Scalar Tensor []>]()\\n  %680 = Div(%678, %679)\\n  %681 = Erf(%680)\\n  %682 = Constant[value = <Scalar Tensor []>]()\\n  %683 = Add(%681, %682)\\n  %684 = Mul(%678, %683)\\n  %685 = Constant[value = <Scalar Tensor []>]()\\n  %686 = Mul(%684, %685)\\n  %686_quantized, %686_scale, %686_zero_point = DynamicQuantizeLinear(%686)\\n  %688_output_quantized = MatMulInteger(%686_quantized, %915_quantized, %686_zero_point, %915_zero_point)\\n  %688_output_quantized_cast_output = Cast[to = 1](%688_output_quantized)\\n  %MatMul_422_quant_scales_mul:0 = Mul(%686_scale, %915_scale)\\n  %688 = Mul(%688_output_quantized_cast_output, %MatMul_422_quant_scales_mul:0)\\n  %689 = Add(%688, %distilbert.transformer.layer.4.ffn.lin2.bias)\\n  %690 = Add(%689, %675)\\n  %691 = ReduceMean[axes = [-1]](%690)\\n  %692 = Sub(%690, %691)\\n  %693 = Constant[value = <Scalar Tensor []>]()\\n  %694 = Pow(%692, %693)\\n  %695 = ReduceMean[axes = [-1]](%694)\\n  %696 = Constant[value = <Scalar Tensor []>]()\\n  %697 = Add(%695, %696)\\n  %698 = Sqrt(%697)\\n  %699 = Div(%692, %698)\\n  %700 = Mul(%699, %distilbert.transformer.layer.4.output_layer_norm.weight)\\n  %701 = Add(%700, %distilbert.transformer.layer.4.output_layer_norm.bias)\\n  %702 = Shape(%701)\\n  %703 = Constant[value = <Scalar Tensor []>]()\\n  %704 = Gather[axis = 0](%702, %703)\\n  %705 = Shape(%701)\\n  %706 = Constant[value = <Scalar Tensor []>]()\\n  %707 = Gather[axis = 0](%705, %706)\\n  %701_quantized, %701_scale, %701_zero_point = DynamicQuantizeLinear(%701)\\n  %709_output_quantized = MatMulInteger(%701_quantized, %916_quantized, %701_zero_point, %916_zero_point)\\n  %709_output_quantized_cast_output = Cast[to = 1](%709_output_quantized)\\n  %MatMul_442_quant_scales_mul:0 = Mul(%701_scale, %916_scale)\\n  %709 = Mul(%709_output_quantized_cast_output, %MatMul_442_quant_scales_mul:0)\\n  %710 = Add(%709, %distilbert.transformer.layer.5.attention.q_lin.bias)\\n  %714 = Unsqueeze[axes = [0]](%704)\\n  %718 = Concat[axis = 0](%714, %917, %918, %919)\\n  %719 = Reshape(%710, %718)\\n  %720 = Transpose[perm = [0, 2, 1, 3]](%719)\\n  %722_output_quantized = MatMulInteger(%701_quantized, %920_quantized, %701_zero_point, %920_zero_point)\\n  %722_output_quantized_cast_output = Cast[to = 1](%722_output_quantized)\\n  %MatMul_448_quant_scales_mul:0 = Mul(%701_scale, %920_scale)\\n  %722 = Mul(%722_output_quantized_cast_output, %MatMul_448_quant_scales_mul:0)\\n  %723 = Add(%722, %distilbert.transformer.layer.5.attention.k_lin.bias)\\n  %727 = Unsqueeze[axes = [0]](%704)\\n  %731 = Concat[axis = 0](%727, %921, %922, %923)\\n  %732 = Reshape(%723, %731)\\n  %734_output_quantized = MatMulInteger(%701_quantized, %924_quantized, %701_zero_point, %924_zero_point)\\n  %734_output_quantized_cast_output = Cast[to = 1](%734_output_quantized)\\n  %MatMul_453_quant_scales_mul:0 = Mul(%701_scale, %924_scale)\\n  %734 = Mul(%734_output_quantized_cast_output, %MatMul_453_quant_scales_mul:0)\\n  %735 = Add(%734, %distilbert.transformer.layer.5.attention.v_lin.bias)\\n  %739 = Unsqueeze[axes = [0]](%704)\\n  %743 = Concat[axis = 0](%739, %925, %926, %927)\\n  %744 = Reshape(%735, %743)\\n  %745 = Transpose[perm = [0, 2, 1, 3]](%744)\\n  %746 = Constant[value = <Scalar Tensor []>]()\\n  %747 = Div(%720, %746)\\n  %748 = Transpose[perm = [0, 2, 3, 1]](%732)\\n  %747_quantized, %747_scale, %747_zero_point = DynamicQuantizeLinear(%747)\\n  %748_quantized, %748_scale, %748_zero_point = DynamicQuantizeLinear(%748)\\n  %749_output_quantized = MatMulInteger(%747_quantized, %748_quantized, %747_zero_point, %748_zero_point)\\n  %749_output_quantized_cast_output = Cast[to = 1](%749_output_quantized)\\n  %MatMul_462_quant_scales_mul:0 = Mul(%747_scale, %748_scale)\\n  %749 = Mul(%749_output_quantized_cast_output, %MatMul_462_quant_scales_mul:0)\\n  %750 = Constant[value = <Scalar Tensor []>]()\\n  %751 = Equal(%attention_mask, %750)\\n  %754 = Unsqueeze[axes = [0]](%704)\\n  %757 = Unsqueeze[axes = [0]](%707)\\n  %758 = Concat[axis = 0](%754, %928, %929, %757)\\n  %759 = Reshape(%751, %758)\\n  %760 = Shape(%749)\\n  %761 = Expand(%759, %760)\\n  %762 = Cast[to = 9](%761)\\n  %763 = Constant[value = <Scalar Tensor []>]()\\n  %764 = Where(%762, %763, %749)\\n  %765 = Softmax[axis = 3](%764)\\n  %765_quantized, %765_scale, %765_zero_point = DynamicQuantizeLinear(%765)\\n  %745_quantized, %745_scale, %745_zero_point = DynamicQuantizeLinear(%745)\\n  %766_output_quantized = MatMulInteger(%765_quantized, %745_quantized, %765_zero_point, %745_zero_point)\\n  %766_output_quantized_cast_output = Cast[to = 1](%766_output_quantized)\\n  %MatMul_475_quant_scales_mul:0 = Mul(%765_scale, %745_scale)\\n  %766 = Mul(%766_output_quantized_cast_output, %MatMul_475_quant_scales_mul:0)\\n  %767 = Transpose[perm = [0, 2, 1, 3]](%766)\\n  %770 = Unsqueeze[axes = [0]](%704)\\n  %773 = Concat[axis = 0](%770, %930, %931)\\n  %774 = Reshape(%767, %773)\\n  %774_quantized, %774_scale, %774_zero_point = DynamicQuantizeLinear(%774)\\n  %776_output_quantized = MatMulInteger(%774_quantized, %932_quantized, %774_zero_point, %932_zero_point)\\n  %776_output_quantized_cast_output = Cast[to = 1](%776_output_quantized)\\n  %MatMul_480_quant_scales_mul:0 = Mul(%774_scale, %932_scale)\\n  %776 = Mul(%776_output_quantized_cast_output, %MatMul_480_quant_scales_mul:0)\\n  %777 = Add(%776, %distilbert.transformer.layer.5.attention.out_lin.bias)\\n  %778 = Add(%777, %701)\\n  %779 = ReduceMean[axes = [-1]](%778)\\n  %780 = Sub(%778, %779)\\n  %781 = Constant[value = <Scalar Tensor []>]()\\n  %782 = Pow(%780, %781)\\n  %783 = ReduceMean[axes = [-1]](%782)\\n  %784 = Constant[value = <Scalar Tensor []>]()\\n  %785 = Add(%783, %784)\\n  %786 = Sqrt(%785)\\n  %787 = Div(%780, %786)\\n  %788 = Mul(%787, %distilbert.transformer.layer.5.sa_layer_norm.weight)\\n  %789 = Add(%788, %distilbert.transformer.layer.5.sa_layer_norm.bias)\\n  %789_quantized, %789_scale, %789_zero_point = DynamicQuantizeLinear(%789)\\n  %791_output_quantized = MatMulInteger(%789_quantized, %933_quantized, %789_zero_point, %933_zero_point)\\n  %791_output_quantized_cast_output = Cast[to = 1](%791_output_quantized)\\n  %MatMul_494_quant_scales_mul:0 = Mul(%789_scale, %933_scale)\\n  %791 = Mul(%791_output_quantized_cast_output, %MatMul_494_quant_scales_mul:0)\\n  %792 = Add(%791, %distilbert.transformer.layer.5.ffn.lin1.bias)\\n  %793 = Constant[value = <Scalar Tensor []>]()\\n  %794 = Div(%792, %793)\\n  %795 = Erf(%794)\\n  %796 = Constant[value = <Scalar Tensor []>]()\\n  %797 = Add(%795, %796)\\n  %798 = Mul(%792, %797)\\n  %799 = Constant[value = <Scalar Tensor []>]()\\n  %800 = Mul(%798, %799)\\n  %800_quantized, %800_scale, %800_zero_point = DynamicQuantizeLinear(%800)\\n  %802_output_quantized = MatMulInteger(%800_quantized, %934_quantized, %800_zero_point, %934_zero_point)\\n  %802_output_quantized_cast_output = Cast[to = 1](%802_output_quantized)\\n  %MatMul_504_quant_scales_mul:0 = Mul(%800_scale, %934_scale)\\n  %802 = Mul(%802_output_quantized_cast_output, %MatMul_504_quant_scales_mul:0)\\n  %803 = Add(%802, %distilbert.transformer.layer.5.ffn.lin2.bias)\\n  %804 = Add(%803, %789)\\n  %805 = ReduceMean[axes = [-1]](%804)\\n  %806 = Sub(%804, %805)\\n  %807 = Constant[value = <Scalar Tensor []>]()\\n  %808 = Pow(%806, %807)\\n  %809 = ReduceMean[axes = [-1]](%808)\\n  %810 = Constant[value = <Scalar Tensor []>]()\\n  %811 = Add(%809, %810)\\n  %812 = Sqrt(%811)\\n  %813 = Div(%806, %812)\\n  %814 = Mul(%813, %distilbert.transformer.layer.5.output_layer_norm.weight)\\n  %815 = Add(%814, %distilbert.transformer.layer.5.output_layer_norm.bias)\\n  %816 = Constant[value = <Scalar Tensor []>]()\\n  %817 = Gather[axis = 1](%815, %816)\\n  %817_quantized, %817_scale, %817_zero_point = DynamicQuantizeLinear(%817)\\n  %818_MatMul_output_quantized = MatMulInteger(%817_quantized, %pre_classifier.weight_quantized, %817_zero_point, %pre_classifier.weight_zero_point)\\n  %818_MatMul_output_quantized_cast_output = Cast[to = 1](%818_MatMul_output_quantized)\\n  %Gemm_520_MatMul_quant_scales_mul:0 = Mul(%817_scale, %pre_classifier.weight_scale)\\n  %818_MatMul = Mul(%818_MatMul_output_quantized_cast_output, %Gemm_520_MatMul_quant_scales_mul:0)\\n  %818 = Add(%818_MatMul, %pre_classifier.bias)\\n  %819 = Relu(%818)\\n  %819_quantized, %819_scale, %819_zero_point = DynamicQuantizeLinear(%819)\\n  %logits_MatMul_output_quantized = MatMulInteger(%819_quantized, %classifier.weight_quantized, %819_zero_point, %classifier.weight_zero_point)\\n  %logits_MatMul_output_quantized_cast_output = Cast[to = 1](%logits_MatMul_output_quantized)\\n  %Gemm_522_MatMul_quant_scales_mul:0 = Mul(%819_scale, %classifier.weight_scale)\\n  %logits_MatMul = Mul(%logits_MatMul_output_quantized_cast_output, %Gemm_522_MatMul_quant_scales_mul:0)\\n  %logits = Add(%logits_MatMul, %classifier.bias)\\n  return %logits\\n}'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052833195
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\r\n",
        "\r\n",
        "# Load the Quantized ONNX Runtime\r\n",
        "ort_session = ort.InferenceSession(\"watchdog_model_quantized_demo.onnx\")"
      ],
      "outputs": [],
      "execution_count": 27,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052834272
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load validation data, cannot be in tensor form\r\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \r\n",
        "                                                            random_state=2018, test_size=0.1)\r\n",
        "\r\n",
        "# Do the same for the masks.\r\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\r\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052834464
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking variables \r\n",
        "predictions , true_labels = [], []\r\n",
        "\r\n",
        "for i in range(len(validation_inputs)):\r\n",
        "\r\n",
        "    cur_input_ids = [validation_inputs[i]]\r\n",
        "    cur_mask = [validation_masks[i]]\r\n",
        "\r\n",
        "    input_dict = {\"input_ids\" : cur_input_ids, \"attention_mask\" : cur_mask}\r\n",
        "\r\n",
        "    outputs = ort_session.run([\"logits\"], input_dict)\r\n",
        "    logits = outputs[0]\r\n",
        "    flat_prediction = np.argmax(logits, axis=1)[0]\r\n",
        "\r\n",
        "    # Store predictions and true labels\r\n",
        "    predictions.append(flat_prediction)\r\n",
        "    true_labels.append(validation_labels[i])"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052834765
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate final accuracy and MCC score\r\n",
        "#\r\n",
        "# If following this notebook cell-for-cell, the observed predictive performance will be lower than\r\n",
        "# that of the production model due to the small size of the sample training dataset used for demo purposes.\r\n",
        "#\r\n",
        "# At this cell, the production ONNX model achieves 0.933 MCC and 0.967 accuracy\r\n",
        "\r\n",
        "from sklearn.metrics import matthews_corrcoef\r\n",
        "print('MCC: %.6f' % matthews_corrcoef(true_labels, predictions))\r\n",
        "\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "print('Accuracy: %.6f' % accuracy_score(true_labels, predictions))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MCC: 1.000000\n",
            "Accuracy: 1.000000\n"
          ]
        }
      ],
      "execution_count": 30,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1630052834970
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}